---
title       : "Inferential Statistics:"
subtitle    : "Advanced Correlational Techniques"
author      : "William R. Buchanan, Ph.D." 
job         : "Strategic Data Fellow at MDE/Adjunct Prof at JSU"
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : []            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides

---


## Overview 
* Announcements
* Review
* What are advanced correlational techniques?
	+ Examples of different types of correlations
	+ Why are they so advanced
* Applying Biserial Correlations to the Classroom
    + How should your test data be stored? 
    + Setting up/Importing your data
    + Generating Total Scores
    + Getting the Point Biserial Correlations
    + What is a bootstrap and why should you use it?
* Next Class

---   

## Announcements
* The task 3 resubmission is due by __this friday at midnight__. 
* I can't tell if blackboard will allow you to resubmit or not, but if you have difficulties resubmitting, email the file with your code to me using the email address in the syllabus
* If you do not email your work to me, I will assume that what is in blackboard is your submission
* __READ THE SYLLABUS__ - many of you are throwing away points by not following the directions listed in the syllabus

---

## Review
* Give an example of when you would use an independent vs a dependent samples t-test.
* When would it make sense to use a dependent samples t-test?
* Given an example of a hypothesis that you would test with a dependent samples t-test.
* List some of the assumptions of a t-test and how you would check the assumptions
* Do the dependent and independent samples t-tests have exactly the same assumptions?  Explain your answer.


---   


```r
# Install these packages if you haven't done so already
# install.packages("psych", dep = TRUE)
# install.packages("ltm", dep = TRUE)

# Load the ltm and boot packages in R
library("ltm"); library("boot"); library("ggplot2"); library("psych")

# Set the random number seed to generate exactly the same data as I am using
set.seed(7779311)

# Simulate a small number of test items for an average size class
test <- sim.rasch(nvar = 20, n = 28, low = -4, high = 4, a = 1, 
					 mu = -0.125, sd = 1)

# Create a data frame of the item responses and add an ID column at the beginning
testData <- cbind(studentid = seq(1, 28, 1), as.data.frame(test$items))
```

---   

# What are advanced correlational techniques?
## Examples of different types of correlations
* Last class we discussed Pearson's _r_ correlation coefficient which is used 
when you want to estimate the relationship between two continuous variables.  
* However, this method is not always appropriate or even the most accurate when 
the two variables you are using are not measured on an intervallic or ratio 
measurement scale


---   

* The following table shows the different types of correlations that are used 
when one variable is measured on the measurement scale listed in the left-most 
row and the other variable is measured on the measurement scale identified in 
the column header

```
## 
## ----------------- -------------- ------------- -----------------
##                      Nominal        Ordinal    Intervallic/Ratio
##      Nominal       Tetrachoric   Rank Biserial  Point Biserial  
##      Ordinal      Rank Biserial   Polychoric      Polyserial    
## Intervallic/Ratio Point Biserial  Polyserial      Pearson's r   
## ----------------- -------------- ------------- -----------------
```


---   

## Why are they so advanced
* A major difference in these correlation methods compared to Pearson's _r_ is 
that these make assumptions about the non-continuous variables
	+ More specifically, they typically assume that there is some underlying 
	trait that is normally distributed that could be used to define thresholds
	+ Pearson's _r_ does not make these same assumptions
* These are not as easy to accurately interpret, but much of what you worked on 
last week could still be applied in a general sense
* The advanced methods that have regular practical use cases in education are 
what we'll be talking about today

---   

* First, however, make sure that you've run the example code from the earlier 
slides as it will be important in a few moments
* Assuming you've already run that code, the next thing to do would be to look 
at your data:


```r
View(testData)
```

---   

# Applying Biserial Correlations to the Classroom
## How should your test data be stored? 
* As you can see from the example from the previous slide, test data should be 
stored with a single item per column and a single student per row
* Although the example above simulates data, you can easily store your data in a 
text file and read it into R to use/store the data.
* If you want to do this with multiple assignments and store/access all of the 
data in R, it would be helpful for you to look at `?rbind` to see how you can 
add the data sets on top of one another

---  

# Example of text dataset
## The example data below are also on GitHub (textexample.txt)
studentid, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10 

1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0 

2, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1 

3, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1 

4, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0 

---   

## Setting up/Importing your data

```r
# Read the text file into the data object called textExample
# There is a HEADER row with variable/column names
# Columns are SEParated by COMMAS (",")
# If there are string/text variables do not convert them to factor variables
textExample <- #
	read.table("~/Desktop/Dropbox/JSU/Fall2014/20141118/textexample.txt", #
	header = TRUE, sep = ",", stringsAsFactors = FALSE)
```
* You can confirm that all went well using `View(textExample)` to look at the data

---   

## Generating Total Scores
* We have data that contain __item responses__ that indicate if the child answered
correctly (1) or incorrectly (0) available now
* What we don't have is a summative measure (e.g., a single score) that describes 
the test performance overall
	+ Although you may normally use the % correct in your grading, it would not 
	be as helpful in this instance
	+ Generating the total score can be fairly easy if you know the structure of 
	your data
* We'll add a new variable to the data to show how to create and store the total 
in the same data object

---   


```r
# Create a column with the total score based on correct responses
testData$total <- rowSums(testData[, -1])
# You can use boxplots to look at the relationship between items and scores
ggplot(testData, aes(x = factor(V13), y = total)) + geom_boxplot()
```

<img src="assets/fig/unnamed-chunk-5.png" title="plot of chunk unnamed-chunk-5" alt="plot of chunk unnamed-chunk-5" style="display: block; margin: auto;" />

---   

* What do you notice about the box plot?  
	+ From this plot, do you think there is a correlation between the total score
	and the response of a student on item 13?
	+ If you do think there is a correlation, is it positive or negative?  And 
	how strong is the correlation (e.g., does a good job of predicting the total)?
	+ If you don't think there is a correlation, why do you think that?
* Even with only 28 students and a 20 item test, there is a lot that you can 
learn about the quality of your test items just from these descriptive statistics

---   

## Getting the Point Biserial Correlations
* So, you probably want to know whether the item was correlated.  
* To do this, we'll use the `biserial()` function from the `library(psych)` package

```r
# The first variable in the function should be your total score The second
# variable in the function MUST BE a DICHOTOMOUS (e.g., 0 or 1) variable
biserial(testData$total, testData$V13)
```

```
##        [,1]
## [1,] 0.6517
```
* But given our sample size we probably want something a bit more robust to make 
any substantive decisions, to do that, I'll briefly show you an advanced technique 
used in statistics called bootstrapping

---   

## What is a bootstrap and why should you use it?
* Hopefully, you remember the different discussions that we've had about how 
larger sample sizes reduce variance (if not you may want to review your notes)
* Bootstrapping is a way to deal with parameters whose true distribution is not 
yet well defined/understood and/or to provide some way to quantify the potential 
bias in your parameter estimates
	+ Since your classes don't have that many students, this can give you a bit 
	better of an understanding of how accurate your correlations are and give you
	a better indication of whether or not to consider using the item in the future
* Keep in mind, however, that this is an extremely cursory look into the world of 
psychometrics and test development
* Most of the assessments you'll encounter from many vendors use much more 
sophisticated modeling techniques, but it all starts with the biserial correlations

---   


```r
# Define a function that the boot strap will calculate repeatedly
f <- function(dF, i) {
    
    # This line allows the boot strap procedure to sample randomly from your
    # data
    dataSample <- dF[i, ]
    
    # We want to return the value of the biserial correlation for the group with
    # the correct responses (level = 1 would give you the group with incorrect
    # responses); change dataSample$V13 to dataSample$V# to use this function
    # for other test items
    return(biserial.cor(dataSample$total, dataSample$V13, level = 2))
    
}  # The function call needs to end with a curly bracket by itself
```

---   


```r
# Now that the function is defined lets use it to get the correlation with
# 10 boot strapped samples
item.10 <- boot(testData, f, R = 10)

# This time with 100 samples
item.100 <- boot(testData, f, R = 100)

# This time with 1000 samples
item.1000 <- boot(testData, f, R = 1000)

# This time with 10000 samples
item.10000 <- boot(testData, f, R = 10000)
```

---   


```r
# Print the results from 10 Bootstrapped samples
item.10
```

```
## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = testData, statistic = f, R = 10)
## 
## 
## Bootstrap Statistics :
##     original   bias    std. error
## t1*   0.5074 -0.02878        0.13
```

--- 


```r
# Print the results from 100 Bootstrapped samples
item.100
```

```
## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = testData, statistic = f, R = 100)
## 
## 
## Bootstrap Statistics :
##     original  bias    std. error
## t1*   0.5074 0.01213      0.1279
```

--- 


```r
# Print the results from 1000 Bootstrapped samples
item.1000
```

```
## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = testData, statistic = f, R = 1000)
## 
## 
## Bootstrap Statistics :
##     original    bias    std. error
## t1*   0.5074 9.273e-05      0.1331
```

--- 


```r
# Print the results from 10000 Bootstrapped samples
item.10000
```

```
## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = testData, statistic = f, R = 10000)
## 
## 
## Bootstrap Statistics :
##     original    bias    std. error
## t1*   0.5074 -0.004165      0.1345
```

---

## Next Class
* Enjoy thanksgiving and the week off.
* Make sure to get Task 4 turned in on time
* For the final, you'll need to do a little bit of everything from the course, 
so make sure you're reviewing things regularly



